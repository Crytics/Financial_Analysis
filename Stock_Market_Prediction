############################################
# Image Recognition in Python
# ------------------------------------------
# Created by  : Adam Nguyen
# Updated by  : Adam Nguyen
# Created at  : 05/12/2015
# Updated at  : xx/xx/xxxx
# Description : Classification
############################################
#http://scikit-learn.org/stable/tutorial/basic/tutorial.html

##Download Appripriate Packages
import cPickle
import numpy as np
import pandas as pd
import datetime
from sklearn import preprocessing
from datetime import datetime
from sklearn.ensemble import RandomForestClassifier
from sklearn import neighbors
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.svm import SVC
import operator
import pandas.io.data
from sklearn.qda import QDA
import re
from dateutil import parser
import os

#from backtest import Strategy, Portfolio
#https://www.quantstart.com/articles/Research-Backtesting-Environments-in-Python-with-pandas

os.chdir('C:/Users/adam.nguyen/Desktop/iPython')
print(os.getcwd())

##Extract raw data from benchmarks and adds "percentage change" feature
def getStock(symbol, start, end):
    df = pd.io.data.get_data_yahoo(symbol, start, end) #queries yahoo data
    
    df.columns.values[-1] = 'AdjClose' #simplifies column naming
    df.columns = df.columns + '_' + symbol #customizes column with symbol
    df['Return_%s' %symbol] = df['AdjClose_%s' %symbol].pct_change() #selects newly created column to get returns
    
    return df

##Extract raw data and adds "percentage change" feature
def getStockFromQuandl(symbol, name, start, end):
    import Quandl
    df = Quandl.get(symbol, trim_start = start, trim_end = end, authoken = "2WySr22sGXfqoB6Djm4y") #queries Quandl data from database

    df.columns.values[-1] = 'AdjClose'
    df.columns = df.columns + '_' + name
    df['Returns_%s' %name] = df['AdjClose_%s' %name].pct_change()
    
    return df

##Combine extraction methods for 1 and 2
def getStockDataFromWeb(fout, start_string, end_string):
    
    #Parse the web for benchmark values
    start = parser.parse(start_string)
    end = parser.parse(end_string)
    
    nasdaq = getStock('^IXIC', start, end)
    frankfurt = getStock('^GDAXI', start, end)
    london = getStock('^FTSE', start, end)
    paris = getStock('^FCHI', start, end)
    hkong = getStock('^HSI', start, end)
    nikkei = getStock('^N225', start, end)
    australia = getStock('^AXJO', start, end)
    
    djia = getStockFromQuandl("YAHOO/INDEX_DJI", 'Djia', start_string, end_string)
    
    out =  pd.io.data.get_data_yahoo(fout, start, end)
    out.columns.values[-1] = 'AdjClose'
    out.columns = out.columns + '_Out'
    out['Return_Out'] = out['AdjClose_Out'].pct_change()
    
    return [out, nasdaq, djia, frankfurt, london, paris, hkong, nikkei, australia]


##Start feature engieering
def addFeatures(dataframe, adjclose, returns, n):
    
    return_n = adjclose[:] + "Time" + str(n)
    dataframe[return_n] = dataframe[adjclose].pct_change(n) #Multiple Day's Return (Against other days)
    
    roll_n = returns[:] + "RollMean" + str(n)
    dataframe[roll_n] = pd.rolling_mean(dataframe[returns], n) #Return's Moving Average (delta)
    
def applyRollMeanDelayReturns(datasets, delta):

    for dataset in datasets:
        columns = dataset.columns
        adjclose = columns[-2]
        returns = columns[-1]
        for n in range(1, delta):
            addFeatures(dataset, adjclose, returns, n)
            
    return datasets


def mergeDataframes(datasets, index, cut):

    subset = []
    subset = [dataset.iloc[:, index:] for dataset in datasets[1:]]
    
    first = subset[0].join(subset[1:], how = 'outer') #the index is a cutoff for only target variables
    finance = datasets[0].iloc[:, index:].join(first, how = 'left') 
    finance = finance[finance.index > cut]
    return finance

def applyTimeLag(dataset, lags, delta): #Delta selects which columns to lag, and lags determine how many lag columns are created
    
    dataset.Return_Out = dataset.Return_Out.shift(-1)
    maxLag = max(range(lags))
 
    columns = dataset.columns[::(2*max(range(delta))-1)]
    for column in columns:
        for lag in range(lags):
            newcolumn = column + 'lag' + str(lag)
            dataset[newcolumn] = dataset[column].shift(lag)
 
    return dataset.iloc[maxLag:-1,:]

def prepareDataForClassification(dataset, start_test):
    
    le = preprocessing.LabelEncoder()
    
    dataset['Target'] = dataset['Return_Out']
    dataset.Target[dataset.Target >= 0] = 'Up'
    dataset.Target[dataset.Target < 0] = 'Down'
    dataset.Target = le.fit(dataset.Target).transform(dataset.Target)
    
    features = dataset.columns[1:-1]
    x = dataset[features]
    y = dataset.Target
    
    x_train = x[x.index < start_test]
    y_train = y[y.index < start_test]
    
    x_test = x[x.index >= start_test]
    y_test = y[y.index >= start_test]
    
    return x_train, y_train, x_test, y_test

#Ensembler Function
def performClassification(x_train, y_train, x_test, y_test, method, parameters, fout, savemodel):
    
    if method == 'RF':
        return performRFClass(x_train, y_train, x_test, y_test, parameters, fout, savemodel)
    
    elif method == 'KNN':
        return performKNNClass(x_train, y_train, x_test, y_test, parameters, fout, savemodel)
    
    elif method == 'SVM':
        return performSVMClass(x_train, y_train, x_test, y_test, parameters, fout, savemodel)
    
    elif method == 'ADA':
        return performADAClass(x_train, y_train, x_test, y_test, parameters, fout, savemodel)
    
    elif method == 'GTB':
        return performGTBClass(x_train, y_train, x_test, y_test, parameters, fout, savemodel)
    
    elif method == 'QDA':
        return performQDAClass(x_train, y_train, x_test, y_test, parameters, fout, savemodel)


##Machine Learning Algorithms

#Random Forest
def performRFClass(x_train, y_train, x_test, y_test, parameters, fout, savemodel):
    
    clf = RandomForestClassifier(n_estimators = 1000, n_jobs = -1)
    clf.fit(x_train, y_train)
    
    if savemodel == True:
        fname_out = '{}-{}.pickle'.format(fout, datetime.now().date())
        with open(fname_out, 'wb') as f:
            cPickle.dump(clf, f, -1)
            
    accuracy = clf.score(x_test, y_test)
    
    return accuracy

#K-Nearest Neighbors
def performKNNClass(x_train, y_train, x_test, y_test, parameters, fout, savemodel):
    
    clf = neighbors.KNeighborsClassifier()
    clf.fit(x_train, y_train)
    
    if savemodel == True:
        fname_out = '{}-{}.pickle'.format(fout, datetime.now().date())
        with open(fname_out, 'wb') as f:
            cPickle.dump(clf, f, -1)
            
    accuracy = clf.score(x_test, y_test)
    
    return accuracy

#Support Vector Machines
def performSVMClass(x_train, y_train, x_test, y_test, parameters, fout, savemodel):

#    c = parameters[0]
#    g = parameters[1]
    clf = SVC()
    clf.fit(x_train, y_train)
    
    if savemodel == True:
        fname_out = '{}-{}.pickle'.format(fout, datetime.now().date())
        with open(fname_out, 'wb') as f:
            cPickle.dump(clf, f, -1)
            
    accuracy = clf.score(x_test, y_test)
    
    return accuracy

#Adaptive Boosting
def performADAClass(x_train, y_train, x_test, y_test, parameters, fout, savemodel):
    
#    n = parameters[0]
#    l = parameters[1]
    clf = AdaBoostClassifier()
    clf.fit(x_train, y_train)
    
    if savemodel == True:
        fname_out = '{}-{}.pickle'.format(fout, datetime.now().date())
        with open(fname_out, 'wb') as f:
            cPickle.dump(clf, f, -1)
            
    accuracy = clf.score(x_test, y_test)
    
    return accuracy

#Gradient Tree Boosting
def performGTBClass(x_train, y_train, x_test, y_test, parameters, fout, savemodel):
    
    clf = GradientBoostingClassifier(n_estimators = 100)
    clf.fit(x_train, y_train)
    
    if savemodel == True:
        fname_out = '{}-{}.pickle'.format(fout, datetime.now().date())
        with open(fname_out, 'wb') as f:
            cPickle.dump(clf, f, -1)
            
    accuracy = clf.score(x_train, y_train)
    
    return accuracy

#Quadratic Discriminant Analysis
def performQDAClass(x_train, y_train, x_test, y_test, parameters, fout, savemodel):
    
#    def replaceTiny(x):
#        if (abs(x) < 0.0001):
#            x = 0.0001
            
#    x_train = x_train.apply(replaceTiny)
#    x_test = x_test.apply(replaceTiny)
    
    clf = QDA()
    clf.fit(x_train, y_train)
    
    if savemodel == True:
        fname_out = '{}-{}.pickle'.format(fout, datetime.now().date())
        with open(fname_out, 'wb') as f:
            cPickle.dump(clf, f, -1)
            
    accuracy = clf.score(x_test, y_test)
    
    return accuracy

#Cross Validation on n-folds
def performTimeSeriesCV(x_train, y_train, number_folds, algorithm, parameters):
    
    print 'Parameters--------------------->', parameters
    print 'Size train set: ', x_train.shape
    
    k = int(np.floor(float(x_train.shape[0]) / number_folds))
    print 'Size of each fold: ', k
    
    accuracies = np.zeros(number_folds-1)
    
    for i in range(2, number_folds-1):
        print ''
        
        split = float(i-1)/i
        
        print 'Splitting the first ' + str(i) + ' chunks at ' + str(i-1) + '/' + str(i)
        
        x = x_train[:(k*i)]
        y = y_train[:(k*i)]     
        print 'Size of train + test: ', x.shape
        
        index = int(np.floor(x.shape[0] * split))
        
        #Training Folds
        x_trainFolds = x[:index]
        y_trainFolds = y[:index]

        #Testing Folds
        x_testFold = x[index + 1:]
        y_testFold = y[index + 1:]

        accuracies[i-2] = performClassification(x_trainFolds, y_trainFolds, x_testFold, y_testFold, algorithm, parameters, 'GSPC', False)
        
        print 'Accuracy on fold ' + str(i) + ': ', accuracies[i-2]
        
    return accuracies.mean()

x_train = data_prep[0]
y_train = data_prep[1]
number_folds = 10
algorithm = 'QDA'
parameters = []

k = int(np.floor(float(x_train.shape[0]) / number_folds)) #Size of each fold
accuracies = np.zeros(number_folds-1) #returns an array of just zeroes to be filled with scores

print k #fold size
print accuracies


    for i in range(2, number_folds-1):
        print ''
        
        split = float(i-1)/i #Identifies the fold
        
        print 'Splitting the first ' + str(i) + ' chunks at ' + str(i-1) + '/' + str(i)
        
        x = x_train[:(k*i)] #dynamically adjusted training sets based on fold
        y = y_train[:(k*i)]     
        print 'Size of train + test: ', x.shape
        
        index = int(np.floor(x.shape[0] * split)) #Cumulative fold / total folds
        
        #Training Folds
        x_trainFolds = x[:index]
        y_trainFolds = y[:index]

        #Testing Folds
        x_testFold = x[index + 1:]
        y_testFold = y[index + 1:]

        accuracies[i-2] = performClassification(x_trainFolds, y_trainFolds, x_testFold, y_testFold, algorithm, parameters, 'GSPC', False)
        
        print 'Accuracy on fold ' + str(i) + ': ', accuracies[i-2]
        
    return accuracies.mean()

##Feature Selection
def performFeatureSelection(maxdeltas, maxlags, fout, cut, start_test, path_datasets, savemodel, method, folds, parameters):
    
    for maxlag in range(3, maxlags + 2):

        lags = range(2, maxlag)
        print ''
        print '===================================='
        print 'Maximum time lag applied', max(lags)
        print ''

        for maxdelta in range(3, maxdeltas + 2):
            datasets = loadDatasets(path_datasets, fout)
            delta = range(2, maxdelta)
            print 'Delta days accounted: ', max(delta)

            datasets = applyRollMeanDelayedReturns(datasets, delta)
            finance = mergeDataframes(datasets, 6, cut)
            print 'Size of data frame: ', finance.shape
            print 'Number of NaN after merging: ', count_missing(finance)

            finance = finance.interpolate(method = 'linear')
            print 'Number of NaN after time interpolation: ', count_missing(finance)

            finance = finance.fillna(finance.mean())
            print 'Number of NaN after mean interpolation: ', count_missing(finance)

            finance = applyTimeLag(finance, lags, delta)
            print 'Number of NaN after temporal shifting: ', count_missing(finance)
            print 'Size of data frame after feature creation: ', finance.shape

            x_train, y_train, x_test, y_test = prepareDataForClassification(finance, start_test)
            
            print performCV(x_train, y_train, folds, method, parameters, fout, savemodel)
            print ''
            


###Commence Analysis
data_select = performFeatureSelection(7, 7, 'GSPC', '2014-01-01', '2012-01-01', data_raw, True, 'RF', 10, [])

data = getStockDataFromWeb('^GSPC', '2012-01-01', '2015-01-01')

data_roll = applyRollMeanDelayReturns(data, 7)

data_finance = mergeDataframes(data_roll, 6, '2012-01-01')

data_lag = applyTimeLag(data_finance, 5, 6)

data_int = data_lag.interpolate() #Perform linear interpolation for NaN

frame = []
for i in data_int.columns.values:
    frame.append(np.count_nonzero(np.isnan(data_int[i])))
cut = max(frame)

data_cut = data_int[cut:]
np.count_nonzero(np.isnan(data_cut))

data_prep = prepareDataForClassification(data_cut, '2014-01-01')

algo = ['RF', 'KNN', 'SVM', 'ADA', 'GTB', 'QDA']

print data_cut

#data_cut.to_csv('test.csv')
#print data_cut.shape #Shape
#data_cut.iloc[:500, 0] #Target
#data_cut.iloc[:500, 1:] #Core

#data_cut.iloc[501:, 0] #Target
#data_cut.iloc[501:, 1:] #Core

#Zero processing
for k in [0, 2]:
    temp = data_prep[k]
    for i in range(temp.shape[0]):
        for j in range(temp.shape[1]):
            if temp.iloc[i, j] < 0.0001:
                temp.iloc[i, j] = 0.0001

output = []
for i in algo:
    output.append(performTimeSeriesCV(data_prep[0], data_prep[1], 10, i, None))


print output

temp = performClassification(data_prep[0], data_prep[1], data_prep[2], data_prep[3], 'ADA', [], '^GSPC', False)
print temp

performTimeSeriesCV(data_prep[0], data_prep[1], 10, 'QDA', None)

